# compressing-chaos

## Basic Folder Structure

```text
latent-ts-diffusion/
├─ README.md
├─ LICENSE
├─ requirements.txt            # or pyproject.toml
├─ .gitignore
├─ configs/                    # all experiment configs (YAML/JSON)
│  ├─ data/
│  │  ├─ finance_vix_sp500.yaml
│  │  └─ energy_gefcom2014.yaml
│  ├─ model/
│  │  ├─ vae_small.yaml
│  │  ├─ vae_beta.yaml
│  │  ├─ diffusion_latent.yaml
│  │  └─ arima_baseline.yaml
│  ├─ train/
│  │  ├─ train_vae.yaml
│  │  ├─ train_diffusion.yaml
│  │  └─ train_baselines.yaml
│  └─ eval/
│     ├─ eval_discriminative.yaml
│     ├─ eval_predictive.yaml
│     └─ eval_visualization.yaml
├─ data/                       # never commit large raw data
│  ├─ raw/
│  │  ├─ finance/
│  │  └─ energy/
│  └─ processed/
│     ├─ finance_windows/
│     └─ energy_windows/
├─ notebooks/                  # exploration / one-off analysis
│  ├─ 01_data_eda.ipynb
│  ├─ 02_vae_sanity_check.ipynb
│  └─ 03_diffusion_debug.ipynb
├─ src/
│  ├─ __init__.py
│  ├─ data/
│  │  ├─ __init__.py
│  │  ├─ download_finance.py   # yf/FRED/Kaggle downloading
│  │  ├─ download_energy.py
│  │  ├─ preprocessing.py      # log returns, rolling vol, windowing
│  │  └─ datasets.py           # PyTorch Dataset/DataModule classes
│  ├─ models/
│  │  ├─ __init__.py
│  │  ├─ vae.py                # main time-series VAE / β-VAE
│  │  ├─ diffusion_latent.py   # DDPM/DDIM in latent space
│  │  └─ baselines/
│  │     ├─ __init__.py
│  │     ├─ arima.py           # statsmodels/pmdarima wrapper
│  │     ├─ vae_baseline.py    # VAE-only generation baseline
│  │     └─ timegan_wrapper.py # optional: wrapper around TimeGAN repo
│  ├─ training/
│  │  ├─ __init__.py
│  │  ├─ train_vae.py          # CLI/LightningTrainer for VAE
│  │  ├─ train_diffusion.py    # train latent diffusion model
│  │  └─ train_baselines.py    # ARIMA + TimeGAN etc.
│  ├─ evaluation/
│  │  ├─ __init__.py
│  │  ├─ discriminative_score.py  # LSTM classifier, real vs synthetic
│  │  ├─ predictive_score.py      # train-on-synth, test-on-real
│  │  ├─ tsne_pca.py              # 2D projections
│  │  └─ stats_acf_fid.py         # ACF, (Context-)FID if implemented
│  └─ utils/
│     ├─ __init__.py
│     ├─ config.py             # load configs, seed everything
│     ├─ logging.py            # W&B/TensorBoard hooks
│     └─ plot.py               # common plotting helpers
├─ scripts/                    # short entrypoints / shell helpers
│  ├─ download_all_data.py
│  ├─ make_windows.py
│  ├─ run_vae_experiment.sh
│  ├─ run_diffusion_experiment.sh
│  └─ run_all_baselines.sh
├─ experiments/                # generated by runs (in .gitignore)
│  ├─ logs/
│  ├─ checkpoints/
│  └─ results/                 # CSVs with metrics, plots, etc.
└─ reports/
   ├─ paper/
   │  ├─ main.tex              # or main.md / main.docx
   │  └─ figures/
   └─ slides/
      └─ presentation.pptx
```

## Environment Setup

Create virtual environment + activate + install dependencies
```bash
python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
```

Set PYTHONPATH so that src/... imports work:

```bash
export PYTHONPATH=$PWD
```

## Data Pipeline

Download SP500 and VIX daily data (Yahoo Finance) into data/raw/finance/ + Create normalized SP500 log return windows of length 50 + Optionally sanity check the processed dataset:

```bash
python scripts/download_all_data.py
python scripts/make_windows.py --seq_len 50 --dataset_name sp500_logret
python scripts/check_dataset.py --seq_len 50 --dataset_name sp500_logret --split train
```

## VAE Training

Train the time series VAE on SP500 windows:

```bash
python -m src.training.train_vae   --seq_len 50 --dataset_name sp500_logret --batch_size 64 --epochs 30   --latent_dim 16 --hidden_dim 64 --beta 0.05   --run_name vae_mlp_sp500_L50_lat16_beta0p05
```

This writes checkpoints under:
```text
\\\ experiments/checkpoints/vae_mlp_sp500_L50_lat16_beta0p05/ best_vae.pt last_vae.pt \\\
```

## Visualize:

Visualize reconstructions on the test set:

```bash
python scripts/visualize_vae_recon.py   --seq_len 50 --dataset_name sp500_logret   --ckpt experiments/checkpoints/vae_mlp_sp500_L50_lat16_beta0p05/best_vae.pt   --num_examples 4   --save_dir experiments/results/vae_recon_sp500_L50
```

This produces PNGs like vae_recon_idx_000.png that show real vs reconstructed windows.

## Diffusion Training

Train the latent DDPM on VAE latent vectors:

```bash
python -m src.training.train_diffusion   --seq_len 50 --dataset_name sp500_logret   --batch_size 256 --epochs 50   --run_name diffusion_latent_sp500_L50
```

Checkpoints are saved under:
```text
experiments/checkpoints/diffusion_latent_sp500_L50/ 
best_diffusion.pt 
last_diffusion.pt
``` 
## Sampling

Generate synthetic SP500 windows:
- prints basic stats for sampled latents and decoded series 
- saves images like real_vs_gen_000.png, real_vs_gen_001.png, etc.

```bash
python scripts/sample_diffusion_vae.py   --vae_ckpt experiments/checkpoints/vae_mlp_sp500_L50_lat16_beta0p05/best_vae.pt   --diff_ckpt experiments/checkpoints/diffusion_latent_sp500_L50/best_diffusion.pt   --seq_len 50 --dataset_name sp500_logret   --num_samples 6   --save_dir experiments/results/diffusion_latent_sp500_L50